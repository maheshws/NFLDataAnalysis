{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scope of ETL project is as follows:\n",
    "We have NFL player data from Wikipedia , Kaggle and Data.World and would want information on players who made it Pro-Bowl from 2017 to 1997\n",
    "1. So we get the players name(list) from Wikipedia for each year's probowl selection ( Check pages from 2017--1997)\n",
    "2. Once we get the information for 30 years of data , we remove the duplicates\n",
    "3. then we join the player name with information ( csv file and spreadsheet from Kaggle and Data.World) to get additional information on the player.\n",
    "4. We clean the data\n",
    "5. We save the data to a cloud database and expose it as REST API's..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "import requests\n",
    "#import bamboolib as bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and set the chromedriver path\n",
    "from splinter import Browser\n",
    "executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://en.wikipedia.org/wiki/'\n",
    "base_url_forplayer = 'https://en.wikipedia.org'\n",
    "allproplayers_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Eric Dickerson': '1983', 'Mark Bavaro': '1985', 'Gary Zimmerman': '1985', 'Mike Munchak': '1982', 'Dwight Stephenson': '1980', 'Morten Andersen': '1982', 'Jim Arnold': '1983', 'Reggie White': '1983', 'Michael Carter': '1984', 'Fredd Young': '1984', 'Carl Banks': '1984', 'Hanford Dixon': '1981', 'Joey Browner': '1983', 'key_to_update': '1985'}\n"
     ]
    }
   ],
   "source": [
    "# Visit the following URL\n",
    "# Our scope is All-Pro-Team from 1997 to 2017\n",
    "nflseason_year = 1987\n",
    "while nflseason_year < 1988:\n",
    "    url = base_url + str(nflseason_year)+'_All-Pro_Team'\n",
    "    try:\n",
    "        browser.visit(url)\n",
    "        time.sleep(3)\n",
    "        #getting the html content from the browser instance launched using splinter\n",
    "        html=browser.html\n",
    "    #scrape the web page using beautiful soup and lxml parser\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        player_tableColl = soup.find_all('table', class_='wikitable')\n",
    "        for table in player_tableColl:\n",
    "            tr_coll = table.find_all('tr')\n",
    "            for item in tr_coll:\n",
    "                data = item.find_all(['th','td'])\n",
    "                if data:\n",
    "                    try:\n",
    "                        player = data[1].a.text\n",
    "                        player_info = data[1].a['href']\n",
    "                        player1 = data[2].a.text\n",
    "                        player1_info = data[1].a['href']\n",
    "                        #check if these players exist in our players dictionary else append them\n",
    "                        if player not in allproplayers_info:\n",
    "                            allproplayers_info.update({player:player_info})\n",
    "                        if player1 not in allproplayers_info:\n",
    "                            allproplayers_info.update({player1:player_info1})\n",
    "                    except Exception:pass\n",
    "                #print(\"exception\")\n",
    "    except Exception:pass\n",
    "    nflseason_year = nflseason_year + 1\n",
    "    \n",
    "#print the dictionary\n",
    "print(allproplayers_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Eric Dickerson': '1983', 'Mark Bavaro': '1985', 'Gary Zimmerman': '1985', 'Mike Munchak': '1982', 'Dwight Stephenson': '1980', 'Morten Andersen': '1982', 'Jim Arnold': '1983', 'Reggie White': '1983', 'Michael Carter': '1984', 'Fredd Young': '1984', 'Carl Banks': '1984', 'Hanford Dixon': '1981', 'Joey Browner': '1983', 'key_to_update': '1985'}\n"
     ]
    }
   ],
   "source": [
    "#now you have the dictionary of players you have to get all the values and scrape thru those sites\n",
    "draft_info_year = []\n",
    "for value in list(allproplayers_info.values()):\n",
    "    try:\n",
    "        tempurl = base_url_forplayer + str(value)\n",
    "        browser.visit(tempurl)\n",
    "        time.sleep(2)\n",
    "        #getting the html content from the browser instance launched using splinter\n",
    "        html=browser.html\n",
    "        #scrape the web page using beautiful soup and lxml parser\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        # find the player name\n",
    "        player_tbl = soup.find(\"table\", {\"class\": \"infobox vcard\"})\n",
    "        #print(player_tbl)\n",
    "        list_of_table_rows = player_tbl.findAll('tr')\n",
    "        #print(list_of_table_rows)\n",
    "        for item in list_of_table_rows:\n",
    "                data = item.find_all(['th'],string=\"NFL Draft:\")\n",
    "                if data:\n",
    "                    try:\n",
    "                        draft_year = data[0].find_next_sibling('td').find('a')['title'][0:4]\n",
    "                        #update the player information dictionary\n",
    "\n",
    "                    except Exception:pass\n",
    "        key_to_update = [k for k,v in allproplayers_info.items() if v == value]\n",
    "        print(key_to_update)\n",
    "        allproplayers_info.update({key_to_update[0]:draft_year})\n",
    "    except Exception:pass\n",
    "print(allproplayers_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eric Dickerson']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].find_next_sibling('td').find('a')['title'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_combine = pd.read_excel('Resources/NFL Combine Data.xlsx', sheet_name='Combine Results')\n",
    "df_nfldraft = pd.read_excel('Resources/NFL Combine Data.xlsx', sheet_name='nfl_draft')\n",
    "\n",
    "\n",
    "df_combine = df_combine[df_combine['Name'].isin(player_info)]\n",
    "df_combine.shape\n",
    "\n",
    "print(df_nfldraft.columns.values)\n",
    "\n",
    "df_nfldraft.drop(['Unnamed: 0','Unnamed: 34'],axis=1,inplace=True)\n",
    "print(df_nfldraft.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NFLChampsMerged = pd.merge(df_combine, df_nfldraft, left_on='Name', right_on='Player')\n",
    "df_NFLChampsMerged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NFLChampsMerged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_College = df_NFLChampsMerged[['College']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_College = df_College.drop_duplicates()\n",
    "df_College.reset_index(drop=True,inplace=True)\n",
    "df_College.reset_index(inplace=True)\n",
    "df_College.rename(columns={\"index\":\"collegeid\"},inplace=True)\n",
    "df_College"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Players = df_NFLChampsMerged[[\"Player_Id\",\"Name\",\"College\",\"POS\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Players = pd.merge(df_Players,df_College,on=\"College\")\n",
    "df_Players.drop(columns=\"College\",inplace=True)\n",
    "df_Players.reset_index(drop=True,inplace=True)\n",
    "df_Players.rename(columns={\"Player_Id\": \"playerid\", \"Name\":\"name\", \"POS\":\"position\"}, inplace=True)\n",
    "df_Players = df_Players[[\"playerid\",\"name\",\"collegeid\",\"position\"]]\n",
    "df_Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Draft =  df_NFLChampsMerged[[\"Player_Id\",'Year_y', 'Rnd', 'Pick', 'Tm']].copy()\n",
    "df_Draft.rename(columns={\"Player_Id\":\"playerid\",\"Year_y\":\"year\",\"Rnd\":\"round\",\"Pick\":\"pick\",\"Tm\":\"nfl_teamid\"},inplace=True)\n",
    "df_Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PlayerStats = df_NFLChampsMerged[[\"Player_Id\",\"First4AV\",\"Age\", \"To\",\"AP1\", \"PB\",\"St\", \"CarAV\", \"DrAV\",\"G\", \"Cmp\", \"Pass_Att\", \"Pass_Yds\", \"Pass_TD\",\"Pass_Int\", \"Rush_Att\",\"Rush_Yds\", \"Rush_TDs\", \"Rec\", \"Rec_Yds\", \"Rec_Tds\",\"Tkl\", \"Def_Int\", \"Sk\"]].copy()\n",
    "df_PlayerStats.rename(columns={\"Player_Id\":\"playerid\",\"First4AV\":\"first4avg\",\"Age\":\"age\",\"To\":\"to_year\",\"AP1\":\"ap1\",\"PB\":\"pb\",\\\n",
    "                               \"St\":\"st\",\"CarAV\":\"career_avg\",\"DrAV\":\"dr_avg\",\"G\":\"games_count\",\"Cmp\":\"completion\",\\\n",
    "                               \"Pass_Att\":\"pass_att\",\"Pass_Yds\":\"pass_yds\",\"Pass_TD\":\"pass_td\",\"Pass_Int\":\"pass_int\",\"Rush_Att\":\"rush_att\",\\\n",
    "                               \"Rush_Yds\":\"rush_yds\",\"Rush_TDs\":\"rush_tds\",\"Rec\":\"rec\",\"Rec_Yds\":\"rec_yds\",\"Rec_Tds\":\"rec_tds\",\\\n",
    "                               \"Tkl\":\"tackle\",\"Def_Int\":\"def_int\",\"Sk\":\"sk\"},inplace=True)\n",
    "df_PlayerStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_CombineData = df_NFLChampsMerged[['Player_Id','Height (in)', 'Weight (lbs)','Hand Size (in)', 'Arm Length (in)', 'Wonderlic', '40 Yard','Bench Press', 'Vert Leap (in)', 'Broad Jump (in)', 'Shuttle', '3Cone','60Yd Shuttle']].copy()\n",
    "\n",
    "df_CombineData = df_NFLChampsMerged.iloc[:,4:17].copy()\n",
    "df_CombineData.rename(columns={df_CombineData.columns[0]: \"height\",df_CombineData.columns[1]: \"weight\",\\\n",
    "                               df_CombineData.columns[2]: \"handsize\",df_CombineData.columns[3]: \"arm_length\",\\\n",
    "                               df_CombineData.columns[4]: \"wonderlic\",df_CombineData.columns[5]: \"forty_yard\",\\\n",
    "                               df_CombineData.columns[6]: \"bench_press\",df_CombineData.columns[7]: \"vert_leap\",\\\n",
    "                               df_CombineData.columns[8]: \"broad_jump\",df_CombineData.columns[9]: \"shuttle\",\\\n",
    "                               df_CombineData.columns[10]: \"threecone\", df_CombineData.columns[11]: \"sixty_yd_shuttle\",\\\n",
    "                               df_CombineData.columns[12]: \"playerid\"},inplace=True)\n",
    "df_CombineData = df_CombineData[['playerid', 'height', 'weight', 'handsize', 'arm_length', 'wonderlic', 'forty_yard',\n",
    "       'bench_press', 'vert_leap', 'broad_jump', 'shuttle', 'threecone',\n",
    "       'sixty_yd_shuttle']]\n",
    "df_CombineData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_College.rename(columns={\"College\":\"college\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place holder for DB Connection\n",
    "\n",
    "# The SQLAlchemy engine will help manage interactions, including automatically\n",
    "# managing a pool of connections to your database\n",
    "\n",
    "#db = sqlalchemy.create_engine(\n",
    "# db = create_engine(\n",
    "#     # Equivalent URL:\n",
    "#     # postgres+pg8000://\"nfl_da\":\"C0nn2nfd_da#7\"@/\"nfl-da-grp7\"?unix_sock=/cloudsql/\"nfl-da-grp7\"/.s.PGSQL.5432\n",
    "#         sqlalchemy.engine.url.URL(\n",
    "#         drivername='postgres+pg8000',\n",
    "#         username=\"nfl_da\",\n",
    "#         password=\"C0nn2nfd_da#7\",\n",
    "#         database=\"nfl-da-grp7\",\n",
    "#         query={\n",
    "#             'unix_sock': '/cloudsql/{nfl-da-grp7}/.s.PGSQL.5432'.format(cloud_sql_connection_name)\n",
    "#         }\n",
    "#     ),\n",
    "#     # ... Specify additional properties here.\n",
    "#     # ...\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_connection_string = \"postgres:cyclone1@localhost:5432/nfl-da-grp7\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place holder for inserting Dataframe to postgres table\n",
    "\n",
    "df_College.to_sql(name='college', con=engine, if_exists='append', index=False)\n",
    "df_Players.to_sql(name='player', con=engine, if_exists='append', index=False)\n",
    "df_Draft.to_sql(name='draft', con=engine, if_exists='append', index=False)\n",
    "df_PlayerStats.to_sql(name='player_stats', con=engine, if_exists='append', index=False)\n",
    "df_CombineData.to_sql(name='combine_data', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
